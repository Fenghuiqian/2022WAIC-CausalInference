{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361afb85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from econml.sklearn_extensions.linear_model import WeightedLasso\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(2023) \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from tqdm import tqdm\n",
    "import skopt\n",
    "from econml.dml import CausalForestDML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33e3cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score01(train_t1, test_data_t1):\n",
    "    trn = train_t1.reshape(-1, 1)\n",
    "    trn = pd.DataFrame(trn, columns=['effect1'])\n",
    "    tst = pd.DataFrame({'effect1': test_data_t1}) \n",
    "    trn_tst = pd.concat([trn, tst], axis=0, ignore_index=True) \n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    target = target.reset_index(drop=True)\n",
    "    result = pd.concat([target, trn_tst], axis=1) \n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_1 - result.effect1)**2)/result.shape[0])/result.ce_1.mean() \n",
    "        return r \n",
    "    return calc_metric(result) \n",
    "\n",
    "def calc_score02(train_t1, test_data_t1):\n",
    "    trn = train_t1.reshape(-1, 1)\n",
    "    trn = pd.DataFrame(trn, columns=['effect1'])\n",
    "    tst = pd.DataFrame({'effect1': test_data_t1}) \n",
    "    trn_tst = pd.concat([trn, tst], axis=0, ignore_index=True) \n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    target = target.reset_index(drop=True)\n",
    "    result = pd.concat([target, trn_tst], axis=1) \n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_2 - result.effect1)**2)/result.shape[0])/result.ce_2.mean() \n",
    "        return r \n",
    "    return calc_metric(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c7bb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesGridSearchCVParams(X, Y, objective='regression', scoring='neg_mean_absolute_error'):\n",
    "    \"\"\"  \n",
    "    model: lgbm\n",
    "    X, Y dtype: int, float, category\n",
    "    objective:  'regression': 传统的均方误差回归。\n",
    "                'regression_l1': 使用L1损失的回归，也称为 Mean Absolute Error (MAE)。\n",
    "                'huber': 使用Huber损失的回归，这是均方误差和绝对误差的结合，特别适用于有异常值的情况。\n",
    "                'fair': 使用Fair损失的回归，这也是另一种对异常值鲁棒的损失函数。\n",
    "                'binary', \n",
    "                'multiclass'\n",
    "    scoring:\n",
    "    'neg_root_mean_squared_error', 'precision_micro', 'jaccard_micro', 'f1_macro', \n",
    "    'recall_weighted', 'neg_mean_absolute_percentage_error', 'f1_weighted', \n",
    "    'completeness_score', 'neg_brier_score', 'neg_mean_gamma_deviance', 'precision', \n",
    "    'adjusted_mutual_info_score', 'f1_samples', 'jaccard', 'neg_mean_poisson_deviance', \n",
    "    'precision_samples', 'recall', 'recall_samples', 'top_k_accuracy', 'roc_auc_ovr', \n",
    "    'mutual_info_score', 'jaccard_samples', 'positive_likelihood_ratio', 'f1_micro', \n",
    "    'adjusted_rand_score', 'accuracy', 'matthews_corrcoef', 'neg_mean_squared_log_error', \n",
    "    'precision_macro', 'rand_score', 'neg_log_loss', 'recall_macro', 'roc_auc_ovo', \n",
    "    'average_precision', 'jaccard_weighted', 'max_error', 'neg_median_absolute_error', \n",
    "    'jaccard_macro', 'roc_auc_ovo_weighted', 'fowlkes_mallows_score', 'precision_weighted', \n",
    "    'balanced_accuracy', 'v_measure_score', 'recall_micro', 'normalized_mutual_info_score', \n",
    "    'neg_mean_squared_error', 'roc_auc', 'roc_auc_ovr_weighted', 'f1', 'homogeneity_score', \n",
    "    'explained_variance', 'r2', 'neg_mean_absolute_error', 'neg_negative_likelihood_ratio'\n",
    "    \"\"\" \n",
    "    if Y[Y.columns[0]].dtype in (float, int):\n",
    "        y_type = 'regression' if objective is None else objective\n",
    "    elif Y[Y.columns[0]].unique().shape[0]==2:\n",
    "        y_type = 'binary'\n",
    "    elif Y[Y.columns[0]].unique().shape[0] > 2:\n",
    "        y_type = 'multiclass'\n",
    "    else:\n",
    "        raise ValueError('确认Y的类别数')\n",
    "    print(y_type) \n",
    "    # grid \n",
    "    if y_type in ('multiclass', 'binary'): \n",
    "        params = {'boosting_type': 'gbdt', 'objective': y_type,\n",
    "                'class_weight': 'balanced', 'n_jobs': -1} \n",
    "        estimator = lgb.LGBMClassifier(**params) \n",
    "        param_grid = {'learning_rate': Real(0.01, 0.03), \n",
    "                      'n_estimators': Integer(500, 2000),\n",
    "                      'num_leaves': Integer(7, 31), \n",
    "                      'min_child_samples': Integer(3,  11), \n",
    "#                       'reg_alpha': Real(0.0, 0.1),\n",
    "                      'reg_lambda': Real(0, 0.1), \n",
    "                      'seed': Categorical([42])} \n",
    "    else:\n",
    "        params = {'boosting_type': 'gbdt',  'n_jobs': -1}\n",
    "        estimator = lgb.LGBMRegressor(**params) \n",
    "        param_grid = {'objective': Categorical(['regression', 'regression_l1', 'huber', 'fair']),\n",
    "                      'learning_rate': Real(0.01, 0.03), \n",
    "                      'n_estimators': Integer(3000, 4000),\n",
    "                      'num_leaves': Integer(10, 20),\n",
    "                      'min_child_samples': Integer(7, 20),\n",
    "#                       'reg_alpha': 0,\n",
    "#                       'reg_lambda': 0, \n",
    "                      'seed': Categorical([42])} \n",
    "    # search \n",
    "    grid = skopt.BayesSearchCV(estimator, param_grid, \n",
    "                         n_iter=100,\n",
    "                         cv=3, scoring = scoring, n_jobs=-1, verbose=0)\n",
    "    grid.fit(X, Y) \n",
    "    params.update(grid.best_params_)\n",
    "    print('Best parameters found by grid search are:', params)\n",
    "    return params "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce8092",
   "metadata": {},
   "source": [
    "##### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb3b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./dataset/data/best/X.csv',index_col=0)\n",
    "test = pd.read_csv('./dataset/data/best/test.csv', index_col=0)\n",
    "test = test[X.columns]\n",
    "T = pd.read_csv('./dataset/data/best/T.csv')\n",
    "Y = pd.read_csv('./dataset/data/best/Y.csv') \n",
    "T = T.astype(str).astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70aa4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36188, 13), (36188, 14))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_T = pd.concat([X, T], axis=1) \n",
    "X_T_Y = pd.concat([X_T, Y], axis=1) \n",
    "X_T.shape, X_T_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8841524c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15635, 1), (15635, 12), (15635, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_01 = X_T_Y.loc[X_T_Y['T'].isin(['0', '1'])][['T']]\n",
    "Y_01 = X_T_Y.loc[X_T_Y['T'].isin(['0', '1'])][['Y']]\n",
    "X_01 = X_T_Y.loc[X_T_Y['T'].isin(['0', '1'])].drop(columns=['T', 'Y']) \n",
    "X_T_01 = X_T_Y.loc[X_T_Y['T'].isin(['0', '1'])].drop(columns=['Y']) \n",
    "T_01['T'] = T_01['T'].astype(str).astype('category')\n",
    "X_T_01['T'] = X_T_01['T'].astype(str).astype('category')\n",
    "Y_01.shape, X_01.shape, T_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a83dd6",
   "metadata": {},
   "source": [
    "##### residual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47414ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_XT01 = bayesGridSearchCVParams(X_01, T_01, objective='binary', scoring='roc_auc') \n",
    "params_XT01 = {'boosting_type': 'gbdt', 'objective': 'binary', 'class_weight': 'balanced', 'n_jobs': -1, 'learning_rate': 0.03, 'min_child_samples': 7, 'n_estimators': 500, 'num_leaves': 7, 'reg_lambda': 0.05, 'seed': 42}\n",
    "mdl_t01 = lgb.LGBMClassifier(**params_XT01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd50192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_XY01 = bayesGridSearchCVParams(X_01, Y_01, objective='regression', scoring='neg_mean_absolute_error') \n",
    "params_XY01 = {'boosting_type': 'gbdt', 'objective': 'regression',   'n_jobs': -1,  'learning_rate': 0.03, 'min_child_samples': 4, 'n_estimators': 1000,   'num_leaves': 15,  'seed': 42}\n",
    "mdl_y01 = lgb.LGBMRegressor(**params_XY01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084365b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0739759c",
   "metadata": {},
   "source": [
    "### est01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68645508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'min_samples_split': 200, 'max_samples': 0.28, 'max_features': 0.9, 'max_depth': 8} 0.31408646433912396\n"
     ]
    }
   ],
   "source": [
    "max_features_list = [0.9]\n",
    "n_estimators_list = [500]\n",
    "max_depth_list = [8]\n",
    "min_samples_split_list = [200] \n",
    "max_samples_list = [0.28]\n",
    "for max_fea in max_features_list:\n",
    "    for n_est in n_estimators_list:\n",
    "        for max_dep in max_depth_list:\n",
    "            for min_samp in min_samples_split_list:\n",
    "                for max_samp in max_samples_list:\n",
    "                    para_tmp = {'n_estimators': n_est, \n",
    "                               'min_samples_split': min_samp, \n",
    "                               'max_samples': max_samp, \n",
    "                               'max_features': max_fea, \n",
    "                               'max_depth': max_dep} \n",
    "                    est01 = CausalForestDML( \n",
    "                                    model_y = mdl_y01, model_t = mdl_t01,\n",
    "                                    discrete_treatment =True, \n",
    "                                    categories =['0', '1'], \n",
    "                                    mc_iters =4, mc_agg = 'mean', \n",
    "                                    drate =True,\n",
    "                                    criterion ='mse', \n",
    "                                    featurizer = None, \n",
    "                                    max_depth = max_dep, \n",
    "                                    n_estimators = n_est, \n",
    "                                    min_samples_split = min_samp, \n",
    "                                    max_samples = max_samp, \n",
    "                                    honest=True, \n",
    "#                                     min_weight_fraction_leaf = 0.01 ,\n",
    "#                                     min_var_leaf_on_val = False , \n",
    "                                    inference =True,\n",
    "                                    max_features = max_fea,\n",
    "                                    n_jobs =-1, \n",
    "                                    random_state =2023,\n",
    "                                    verbose =0 ) \n",
    "                    est01.fit(Y=Y_01, T=T_01, X=X_01)\n",
    "                    test_t01 = est01.effect(X=test, T0='0', T1='1') \n",
    "                    X_t01 = est01.effect(X=X, T0='0', T1='1')  \n",
    "                    print(para_tmp, calc_score01(X_t01.reshape(-1), test_t01.reshape(-1)))\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3d426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump\n",
    "# dump(est01, './dataset/model/causalForest/causalForest0_31_MODEL_est01.joblib')\n",
    "# pd.DataFrame(test_t01, columns=['test_t01']).to_csv('./dataset/model/causalForest/test_t01.csv', index=False) \n",
    "# pd.DataFrame(X_t01, columns=['X_t01']).to_csv('./dataset/model/causalForest/X_t01.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee75ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1e3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e770fbe",
   "metadata": {},
   "source": [
    "### est02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe9d446e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34474, 1), (34474, 12), (34474, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_02 = X_T_Y.loc[X_T_Y['T'].isin(['0', '2'])][['T']]\n",
    "Y_02 = X_T_Y.loc[X_T_Y['T'].isin(['0', '2'])][['Y']]\n",
    "X_02 = X_T_Y.loc[X_T_Y['T'].isin(['0', '2'])].drop(columns=['T', 'Y']) \n",
    "X_T_02 = X_T_Y.loc[X_T_Y['T'].isin(['0', '2'])].drop(columns=['Y']) \n",
    "T_02['T'] = T_02['T'].astype(str).astype('category')\n",
    "X_T_02['T'] = X_T_02['T'].astype(str).astype('category')\n",
    "Y_02.shape, X_02.shape, T_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32747abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary\n",
      "Best parameters found by grid search are: {'boosting_type': 'gbdt', 'objective': 'binary', 'class_weight': 'balanced', 'n_jobs': -1, 'learning_rate': 0.010618497852749719, 'min_child_samples': 11, 'n_estimators': 500, 'num_leaves': 7, 'reg_lambda': 0.1, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "# params_XT02 = bayesGridSearchCVParams(X_02, T_02, objective='binary', scoring='roc_auc') \n",
    "params_XT02 = {'boosting_type': 'gbdt', 'objective': 'binary', 'class_weight': 'balanced', 'n_jobs': -1, 'learning_rate': 0.010618497852749719, 'min_child_samples': 11, 'n_estimators': 500, 'num_leaves': 7, 'reg_lambda': 0.1, 'seed': 42}\n",
    "mdl_t02 = lgb.LGBMClassifier(**params_XT02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "958c644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression\n",
      "Best parameters found by grid search are: {'boosting_type': 'gbdt', 'n_jobs': -1, 'learning_rate': 0.03, 'min_child_samples': 15, 'n_estimators': 4000, 'num_leaves': 10, 'objective': 'regression_l1', 'reg_lambda': 0, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "# params_XY02 = bayesGridSearchCVParams(X_02, Y_02, objective='regression', scoring='neg_mean_absolute_error') \n",
    "params_XY02 = {'boosting_type': 'gbdt', 'n_jobs': -1, 'learning_rate': 0.03, 'min_child_samples': 15, 'n_estimators': 4000, 'num_leaves': 10, 'objective': 'regression_l1', 'reg_lambda': 0, 'seed': 42}\n",
    "mdl_y02 = lgb.LGBMRegressor(**params_XY02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d087e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa6f5225",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'min_samples_split': 200, 'max_samples': 0.5, 'max_features': 0.9, 'max_depth': 8} 0.04965519571874911\n"
     ]
    }
   ],
   "source": [
    "max_samples_list = [0.5]\n",
    "max_features_list = [0.9]\n",
    "min_samples_split_list = [200]\n",
    "n_estimators_list = [500]\n",
    "max_depth_list = [8] \n",
    "for max_fea in max_features_list:\n",
    "    for max_samp in max_samples_list:\n",
    "        for min_samp in min_samples_split_list:\n",
    "            for n_est in n_estimators_list:\n",
    "                for max_dep in max_depth_list:\n",
    "                    params_tmp = {'n_estimators': n_est, \n",
    "                               'min_samples_split': min_samp, \n",
    "                               'max_samples': max_samp, \n",
    "                               'max_features': max_fea, \n",
    "                               'max_depth': max_dep} \n",
    "\n",
    "                    est02 = CausalForestDML( \n",
    "                                    model_y = mdl_y02, model_t = mdl_t02,\n",
    "                                    discrete_treatment =True, \n",
    "                                    categories =['0', '2'], \n",
    "                                    mc_iters =4, mc_agg = 'mean', \n",
    "                                    drate = True,\n",
    "                                    criterion ='het', \n",
    "                                    featurizer = None,\n",
    "                                    max_depth = max_dep, \n",
    "                                    n_estimators = n_est, \n",
    "                                    min_samples_split = min_samp, \n",
    "                                    max_samples = max_samp,\n",
    "                                    honest=True, \n",
    "#                                     min_weight_fraction_leaf = 0.01 ,\n",
    "#                                     min_var_leaf_on_val = False , \n",
    "                                    inference =True,\n",
    "                                    max_features = max_fea,\n",
    "                                    n_jobs =-1, \n",
    "                                    random_state =2023,\n",
    "                                    verbose =0 ) \n",
    "                    est02.fit(Y=Y_02, T=T_02, X=X_02)\n",
    "                    test_t02 = est02.effect(X=test, T0='0', T1='2') \n",
    "                    X_t02 = est02.effect(X=X, T0='0', T1='2') \n",
    "                    print(params_tmp, calc_score02(X_t02.reshape(-1), test_t02.reshape(-1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea869c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'n_estimators': 500, 'min_samples_split': 200, 'max_samples': 0.5, \n",
    "#  'max_features': 0.9, 'max_depth': 8} 0.04965519571874911\n",
    "# from joblib import dump\n",
    "# dump(est02, './dataset/model/causalForest/causalForest0_0524_MODEL_est02.joblib')\n",
    "\n",
    "# pd.DataFrame(test_t02, columns=['test_t02']).to_csv(\n",
    "#     './dataset/model/causalForest/test_t02.csv', index=False) \n",
    "# pd.DataFrame(X_t02, columns=['X_t02']).to_csv(\n",
    "#     './dataset/model/causalForest/X_t02.csv', index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0c31d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(pred01, pred02):\n",
    "    result = pd.DataFrame(np.concatenate((pred01.reshape(-1, 1), pred02.reshape(-1, 1)), axis=1), \n",
    "                    columns=['pred01', 'pred02'])\n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    result = pd.concat([target, result], axis=1)\n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_1 - result.pred01)**2)/result.shape[0])/result.ce_1.mean() + \\\n",
    "            np.sqrt(np.sum((result.ce_2 - result.pred02)**2)/result.shape[0])/result.ce_2.mean()\n",
    "        return r \n",
    "    return calc_metric(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1f51b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36374166005787306"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_score(np.concatenate((X_t01, test_t01), axis=0), np.concatenate((X_t02, test_t02), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd0aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95466d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econml",
   "language": "python",
   "name": "econml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
