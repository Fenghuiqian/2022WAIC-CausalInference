{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361afb85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from econml.orf import DMLOrthoForest\n",
    "from econml.orf import DMLOrthoForest\n",
    "from econml.sklearn_extensions.linear_model import WeightedLasso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(2023) \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error \n",
    "import os\n",
    "from joblib import dump, load \n",
    "from econml.orf import DMLOrthoForest \n",
    "from econml.dr import DRLearner, ForestDRLearner \n",
    "from sklearn.ensemble import GradientBoostingRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eceed93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e3cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score01(train_t1, test_data_t1):\n",
    "    trn = train_t1.reshape(-1, 1)\n",
    "    trn = pd.DataFrame(trn, columns=['effect1'])\n",
    "    tst = pd.DataFrame({'effect1': test_data_t1}) \n",
    "    trn_tst = pd.concat([trn, tst], axis=0, ignore_index=True) \n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    target = target.reset_index(drop=True)\n",
    "    result = pd.concat([target, trn_tst], axis=1) \n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_1 - result.effect1)**2)/result.shape[0])/result.ce_1.mean() \n",
    "        return r \n",
    "    return calc_metric(result) \n",
    "\n",
    "def calc_score02(train_t1, test_data_t1):\n",
    "    trn = train_t1.reshape(-1, 1)\n",
    "    trn = pd.DataFrame(trn, columns=['effect1'])\n",
    "    tst = pd.DataFrame({'effect1': test_data_t1}) \n",
    "    trn_tst = pd.concat([trn, tst], axis=0, ignore_index=True) \n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    target = target.reset_index(drop=True)\n",
    "    result = pd.concat([target, trn_tst], axis=1) \n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_2 - result.effect1)**2)/result.shape[0])/result.ce_2.mean() \n",
    "        return r \n",
    "    return calc_metric(result) \n",
    "\n",
    "def calc_score01(train_t1, test_data_t1):\n",
    "    trn = train_t1.reshape(-1, 1)\n",
    "    trn = pd.DataFrame(trn, columns=['effect1'])\n",
    "    tst = pd.DataFrame({'effect1': test_data_t1.reshape(-1)}) \n",
    "    trn_tst = pd.concat([trn, tst], axis=0, ignore_index=True) \n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    target = target.reset_index(drop=True)\n",
    "    result = pd.concat([target, trn_tst], axis=1) \n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_1 - result.effect1)**2)/result.shape[0])/result.ce_1.mean() \n",
    "        return r \n",
    "    return calc_metric(result) \n",
    "\n",
    "def calc_score02(train_t1, test_data_t1):\n",
    "    trn = train_t1.reshape(-1, 1)\n",
    "    trn = pd.DataFrame(trn, columns=['effect1'])\n",
    "    tst = pd.DataFrame({'effect1': test_data_t1.reshape(-1)}) \n",
    "    trn_tst = pd.concat([trn, tst], axis=0, ignore_index=True) \n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    target = target.reset_index(drop=True)\n",
    "    result = pd.concat([target, trn_tst], axis=1) \n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_2 - result.effect1)**2)/result.shape[0])/result.ce_2.mean() \n",
    "        return r \n",
    "    return calc_metric(result) \n",
    "\n",
    "def calc_score(pred01, pred02):\n",
    "    result = pd.DataFrame(np.concatenate((pred01.reshape(-1, 1), pred02.reshape(-1, 1)), axis=1), \n",
    "                    columns=['pred01', 'pred02'])\n",
    "    target = pd.read_csv('./dataset/data/target.csv')\n",
    "    result = pd.concat([target, result], axis=1)\n",
    "    def calc_metric(result):\n",
    "        r = np.sqrt(np.sum((result.ce_1 - result.pred01)**2)/result.shape[0])/result.ce_1.mean() + \\\n",
    "            np.sqrt(np.sum((result.ce_2 - result.pred02)**2)/result.shape[0])/result.ce_2.mean()\n",
    "        return r \n",
    "    return calc_metric(result) \n",
    "\n",
    "def optunaSearchCVParams_LGBM(X, Y, cv=6, n_iter=30, sampler='tpe',  study_name = 'new',\n",
    "                              objective_type='binary',  scoring='average_precision', direction='maximize',\n",
    "                             n_jobs_est=10, n_jobs_optuna=3, use_gpu=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    direction:  'minimize', \n",
    "                'maximize'\n",
    "    objective_type: binary, multiclass, \n",
    "    \n",
    "    scoring: binary: 'average_precision','roc_auc'\n",
    "             multiclass: 'log_loss', 'accuracy_score'\n",
    "             \n",
    "             \n",
    "    model: lgbm\n",
    "    X, Y dtype: int, float, category\n",
    "    objective:  'regression': 传统的均方误差回归。\n",
    "                'regression_l1': 使用L1损失的回归，也称为 Mean Absolute Error (MAE)。\n",
    "                'huber': 使用Huber损失的回归，这是均方误差和绝对误差的结合，特别适用于有异常值的情况。\n",
    "                'fair': 使用Fair损失的回归，这也是另一种对异常值鲁棒的损失函数。\n",
    "                'binary', \n",
    "                'multiclass'\n",
    "    scoring:\n",
    "    'neg_root_mean_squared_error', 'precision_micro', 'jaccard_micro', 'f1_macro', \n",
    "    'recall_weighted', 'neg_mean_absolute_percentage_error', 'f1_weighted', \n",
    "    'completeness_score', 'neg_brier_score', 'neg_mean_gamma_deviance', 'precision', \n",
    "    'adjusted_mutual_info_score', 'f1_samples', 'jaccard', 'neg_mean_poisson_deviance', \n",
    "    'precision_samples', 'recall', 'recall_samples', 'top_k_accuracy', 'roc_auc_ovr', \n",
    "    'mutual_info_score', 'jaccard_samples', 'positive_likelihood_ratio', 'f1_micro', \n",
    "    'adjusted_rand_score', 'accuracy', 'matthews_corrcoef', 'neg_mean_squared_log_error', \n",
    "    'precision_macro', 'rand_score', 'neg_log_loss', 'recall_macro', 'roc_auc_ovo', \n",
    "    'average_precision', 'jaccard_weighted', 'max_error', 'neg_median_absolute_error', \n",
    "    'jaccard_macro', 'roc_auc_ovo_weighted', 'fowlkes_mallows_score', 'precision_weighted', \n",
    "    'balanced_accuracy', 'v_measure_score', 'recall_micro', 'normalized_mutual_info_score', \n",
    "    'neg_mean_squared_error', 'roc_auc', 'roc_auc_ovr_weighted', 'f1', 'homogeneity_score', \n",
    "    'explained_variance', 'r2', 'neg_mean_absolute_error', 'neg_negative_likelihood_ratio'\n",
    "\n",
    "             \n",
    "    optuna.samplers.GridSampler(网格搜索采样)\n",
    "    optuna.samplers.RandomSampler(随机搜索采样)\n",
    "    optuna.samplers.TPESampler(贝叶斯优化采样)\n",
    "    optuna.samplers.NSGAIISampler(遗传算法采样)\n",
    "    optuna.samplers.CmaEsSampler(协方差矩阵自适应演化策略采样，非常先进的优化算法)\n",
    "\n",
    "    贝叶斯优化（TPESampler）:\n",
    "    基于过去的试验结果来选择新的参数组合，通常可以更快地找到好的解。\n",
    "    当参数间的依赖关系比较复杂时，可能会更有优势。\n",
    "    \n",
    "    遗传算法（NSGAIISampler）:\n",
    "    遗传算法是一种启发式的搜索方法，通过模拟自然选择过程来探索参数空间。\n",
    "    对于非凸或非线性问题可能表现良好。\n",
    "    \n",
    "    协方差矩阵自适应演化策略（CmaEsSampler）:\n",
    "    CMA-ES是一种先进的优化算法，适用于连续空间的非凸优化问题。\n",
    "    通常需要较多的计算资源，但在一些困难问题上可能会表现得非常好。\n",
    "    \n",
    "    如果你的计算资源充足，网格搜索可能是一个可靠的选择，因为它可以穷举所有的参数组合来找到最优解。\n",
    "    如果你希望在较短的时间内得到合理的解，随机搜索和贝叶斯优化可能是更好的选择。\n",
    "    如果你面临的是一个非常复杂或非线性的问题，遗传算法和CMA-ES可能值得尝试。\n",
    "    \"\"\"\n",
    "    \n",
    "    optuna.logging.set_verbosity(optuna.logging.ERROR) \n",
    "#     def lgb_median_absolute_error(y_true, y_pred):\n",
    "#         return 'median_absolute_error', median_absolute_error(y_true, y_pred), False \n",
    "    \n",
    "#     lb = LabelBinarizer()\n",
    "#     Y_lb = lb.fit_transform(Y) \n",
    "#     def custom_eval_metric(y_true, y_pred):\n",
    "#         predicted_categories = np.argmax(y_pred, axis=1).reshape(-1, 1)\n",
    "#         y_pred_cate = lb.inverse_transform(predicted_categories) \n",
    "        \n",
    "#         y_pred_ = y_pred_cate.copy().astype(np.float64) \n",
    "#         y_true_ = y_true.copy().astype(np.float64)\n",
    "#         return 'median_absolute_error', median_absolute_error(y_true_, y_pred_), False \n",
    "\n",
    "#     def custom_eval_metric2(y_true, y_pred):\n",
    "#         y_pred_ = y_pred.copy().astype(np.float64) \n",
    "#         y_true_ = y_true.copy().astype(np.float64)\n",
    "#         return  mean_absolute_error(y_true_, y_pred_)\n",
    "    \n",
    "    \n",
    "    # tpe params\n",
    "#     tpe_params = TPESampler.hyperopt_parameters()\n",
    "#     tpe_params['n_startup_trials'] = 100 \n",
    "    samplers = {\n",
    "#                 'grid': optuna.samplers.GridSampler(), \n",
    "                'random': optuna.samplers.RandomSampler(), \n",
    "#                 'anneal': optuna.samplers, \n",
    "#                 'tpe': optuna.samplers.TPESampler(**tpe_params), \n",
    "                'tpe': optuna.samplers.TPESampler(), \n",
    "                'cma': optuna.samplers.CmaEsSampler(), \n",
    "                'nsgaii': optuna.samplers.NSGAIISampler()} \n",
    "#     optuna.logging.set_verbosity(optuna.logging.ERROR) \n",
    "    \n",
    "    if isinstance(Y, pd.DataFrame):\n",
    "        Y = Y.values.reshape(-1) \n",
    "    if objective_type == 'regression':\n",
    "        objective_list = ['regression', 'regression_l1', 'quantile']  # ['regression', 'regression_l1', 'quantile','huber', 'mape']\n",
    "    elif objective_type == 'binary':\n",
    "        objective_list = ['binary'] \n",
    "    elif objective_type == 'multiclass':\n",
    "        objective_list = ['softmax', 'multiclassova'] \n",
    "        \n",
    "    def objective(trial): \n",
    "        #params\n",
    "        param_grid = { \n",
    "                        'boosting_type': trial.suggest_categorical(\"boosting_type\", ['gbdt', 'dart', 'rf']), # 'gbdt', 'dart', 'rf'\n",
    "                        'objective': trial.suggest_categorical(\"objective\", objective_list), \n",
    "                        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 5000), \n",
    "                        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1),\n",
    "                        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 63),\n",
    "                        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 31), \n",
    "                        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 300),\n",
    "                        'min_sum_hessian_in_leaf': trial.suggest_float(\"min_sum_hessian_in_leaf\", 1e-8, 5, log=True), \n",
    "                        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10, log=True),\n",
    "                        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10, log=True),\n",
    "                        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 1e-8, 10, log=True),\n",
    "                        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.1, 1, step=0.05),\n",
    "                        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7), \n",
    "                        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.05, 1, step=0.02),\n",
    "                        'feature_fraction_bynode':  trial.suggest_float(\"feature_fraction_bynode\", 0.05, 1, step=0.02),\n",
    "                        'max_bin': trial.suggest_int(\"max_bin\", 63, 511), # 默认255 \n",
    "                        'min_data_in_bin': trial.suggest_int(\"min_data_in_bin\", 1, 20),\n",
    "                        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 50, 50), \n",
    "                        'seed': trial.suggest_int('seed', 0, 0), \n",
    "                        'n_jobs': trial.suggest_int('n_jobs', n_jobs_est, n_jobs_est) \n",
    "                     } \n",
    "        \n",
    "        # CATE COLS \n",
    "        cat_cols = X.select_dtypes(include=['category', 'object']).columns.tolist() \n",
    "        if len(cat_cols)>0: \n",
    "            param_grid.update({\"categorical_features\": trial.suggest_categorical(\"categorical_features\", [cat_cols]),\n",
    "                              'cat_smooth': trial.suggest_float(\"cat_smooth\", 1e-5, 200),\n",
    "                               'cat_l2': trial.suggest_float(\"cat_l2\", 1e-5, 200) \n",
    "                              }) \n",
    "        # other prms \n",
    "        if objective_type == 'regression':\n",
    "            param_grid.update({\"alpha\": trial.suggest_float(\"alpha\", 0.5, 0.5), \n",
    "                              'metric': trial.suggest_categorical(\"metric\", ['', 'mae', 'mse', \n",
    "                                                                             'quantile', 'huber']), \n",
    "                              }) \n",
    "\n",
    "        elif objective_type == 'binary': \n",
    "            param_grid.update({'class_weight': trial.suggest_categorical(\"class_weight\", ['balanced', None]), \n",
    "                              'metric': trial.suggest_categorical(\"metric\", [\n",
    "#                                   '', 'binary_logloss', 'average_precision', \n",
    "                                                                             'auc']) \n",
    "                              }) \n",
    "        elif objective_type == 'multiclass':\n",
    "            param_grid.update({'class_weight': trial.suggest_categorical(\"class_weight\", ['balanced', None]), \n",
    "                              'num_class': trial.suggest_int('num_class', np.unique(Y).shape[0], \n",
    "                                                            np.unique(Y).shape[0]),\n",
    "                              'metric': trial.suggest_categorical(\"metric\", [ 'multi_logloss'])  #  '',  'auc_mu', 'multi_error'\n",
    "                              }) \n",
    "        else: \n",
    "            raise ValueError('objective_type error')\n",
    "            \n",
    "        if use_gpu == True:\n",
    "            gpu_params = {'device_type': trial.suggest_categorical(\"device_type\", ['gpu']),   # cpu, gpu, cuda \n",
    "                          'gpu_platform_id': trial.suggest_categorical(\"gpu_platform_id\", [0]),  \n",
    "                          'gpu_device_id': trial.suggest_categorical(\"gpu_device_id\", [0])} \n",
    "            param_grid.update(gpu_params)\n",
    "        \n",
    "        # est : oof score平均得到\n",
    "#         est_lgb = lgb.LGBMClassifier(**param_grid)\n",
    "#         score = cross_val_score(est_lgb, X, Y, \n",
    "#                                 cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=0), \n",
    "#                                 scoring=scoring).mean() \n",
    "#         return score \n",
    "\n",
    "    \n",
    "        # 交叉验证\n",
    "        if objective_type in ('binary', 'multiclass'):\n",
    "            kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=100) \n",
    "            kf_split = kf.split(X, Y)\n",
    "        else:\n",
    "            kf = KFold(n_splits=cv, shuffle=True, random_state=0) \n",
    "            kf_split = kf.split(X)\n",
    "        \n",
    "        if objective_type in ('binary', 'multiclass'):\n",
    "            if scoring in ('accuracy_score', 'balanced_accuracy_score'): \n",
    "                oof_pred = pd.Series([None]*X.shape[0]) \n",
    "            elif scoring in ('average_precision', 'roc_auc', 'log_loss', 'multi_logloss'):\n",
    "                oof_pred = np.zeros([Y.shape[0], np.unique(Y).shape[0]]) \n",
    "            else:\n",
    "                raise ValueError('check scoring')\n",
    "        else:\n",
    "            oof_pred = np.zeros(X.shape[0]) \n",
    "\n",
    "        for idx, (train_idx, test_idx) in enumerate(kf_split): \n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[test_idx] \n",
    "            y_train, y_val = Y[train_idx], Y[test_idx] \n",
    "            if objective_type in ('binary', 'multiclass'):\n",
    "                # LGBM建模\n",
    "                model = lgb.LGBMClassifier(**param_grid, verbose=-2) \n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "    #                 callbacks=[LightGBMPruningCallback(trial, \"average_precision\")]\n",
    "                        ) \n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(**param_grid, verbose=-2) \n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "    #                 callbacks=[LightGBMPruningCallback(trial, \"average_precision\")]\n",
    "                        )\n",
    "            # 预测\n",
    "            if scoring in ('average_precision', 'roc_auc', 'log_loss', 'multi_logloss'):\n",
    "                y_pred_prob = model.predict_proba(X_val, num_iteration=model.best_iteration_) \n",
    "                oof_pred[test_idx] = y_pred_prob \n",
    "            elif scoring in ('accuracy_score', 'balanced_accuracy_score'): \n",
    "                preds = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "                oof_pred.iloc[test_idx] = preds \n",
    "            elif scoring in ('mean_absolute_error', 'mean_squared_error', 'median_absolute_error'): \n",
    "                preds = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "                oof_pred[test_idx] = preds \n",
    "            else:\n",
    "                raise ValueError('check scoring')\n",
    "        \n",
    "        if scoring == 'average_precision':\n",
    "            lb = LabelBinarizer()\n",
    "            # 字母序encoding\n",
    "            Y_lb = lb.fit_transform(Y) \n",
    "            score = average_precision_score(Y_lb, oof_pred) \n",
    "        elif scoring == 'roc_auc':\n",
    "#             lb = LabelBinarizer() \n",
    "#             Y_lb = lb.fit_transform(Y) \n",
    "#             print(oof_pred)  \n",
    "            score = roc_auc_score(Y, oof_pred[:, 1]) \n",
    "        elif scoring in ('log_loss', 'multi_logloss'):\n",
    "            lb = LabelBinarizer() \n",
    "            Y_lb = lb.fit_transform(Y) \n",
    "            score = log_loss(Y_lb, oof_pred) \n",
    "        elif scoring in ('accuracy_score'):\n",
    "            score = accuracy_score(Y, oof_pred) \n",
    "        elif scoring in ( 'balanced_accuracy_score'):\n",
    "            score = custom_eval_metric2(Y, oof_pred)\n",
    "        elif scoring == 'mean_absolute_error':\n",
    "            score = mean_absolute_error(Y, oof_pred)\n",
    "        elif scoring == 'mean_squared_error':\n",
    "            score = mean_squared_error(Y, oof_pred)\n",
    "        elif scoring == 'median_absolute_error':\n",
    "            score = median_absolute_error(Y, oof_pred) \n",
    "            score2 = mean_absolute_error(Y, oof_pred)\n",
    "        else:\n",
    "            raise ValueError('check scoring')  \n",
    "        return score \n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=direction, \n",
    "                                sampler=samplers[sampler], \n",
    "#                                 pruner=optuna.pruners.HyperbandPruner(max_resource='auto'), \n",
    "#                                 pruner=optuna.pruners.MedianPruner(interval_steps=20, n_min_trials=8),\n",
    "                                storage=optuna.storages.RDBStorage(url=\"sqlite:///db.sqlite3\", \n",
    "                                                                   engine_kwargs={\"connect_args\": {\"timeout\": 500}}),  # 指定database URL \n",
    "                                study_name= '%s_%s_%s'%(study_name, sampler, X.shape[1]), load_if_exists=True) \n",
    "    study.optimize(objective, n_trials=n_iter, show_progress_bar=True, n_jobs=n_jobs_optuna) \n",
    "    print('Best lgbm params:', study.best_trial.params) \n",
    "    print('Best CV score:', study.best_value) \n",
    "    \n",
    "    optuna.visualization.plot_optimization_history(study).show() \n",
    "#     optuna.visualization.plot_intermediate_values(study).show() \n",
    "#     optuna.visualization.plot_parallel_coordinate(study).show() \n",
    "#     optuna.visualization.plot_parallel_coordinate(study, params=[\"max_depth\", \"min_samples_leaf\"]).show() \n",
    "#     optuna.visualization.plot_contour(study).show() \n",
    "#     optuna.visualization.plot_contour(study, params=[\"max_depth\", \"min_samples_leaf\"]).show() \n",
    "#     optuna.visualization.plot_slice(study).show() \n",
    "    optuna.visualization.plot_param_importances(study).show() \n",
    "    params_df = study.trials_dataframe() \n",
    "    params_df = params_df.sort_values(by=['value']) \n",
    "    return study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe3b53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T.csv', 'test.csv', 'X.csv', 'X_01.csv', 'X_02.csv', 'Y.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./dataset/data/best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7ed35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./dataset/data/best/X.csv', index_col=0)\n",
    "X_01 = pd.read_csv('./dataset/data/best/X_01.csv', index_col=0)\n",
    "X_02 = pd.read_csv('./dataset/data/best/X_02.csv', index_col=0)\n",
    "test = pd.read_csv('./dataset/data/best/test.csv', index_col=0)\n",
    "\n",
    "T = pd.read_csv('./dataset/data/best/T.csv')\n",
    "Y = pd.read_csv('./dataset/data/best/Y.csv') \n",
    "T = T.astype(str).astype('category') \n",
    "\n",
    "X_T = pd.concat([X, T], axis=1) \n",
    "X_T_Y = pd.concat([X_T, Y], axis=1) \n",
    "\n",
    "T_01 = X_T_Y.loc[X_T_Y['T'].isin(['0', '1'])][['T']]\n",
    "Y_01 = X_T_Y.loc[X_T_Y['T'].isin(['0', '1'])][['Y']]\n",
    "\n",
    "T_02 = X_T_Y.loc[X_T_Y['T'].isin(['0', '2'])][['T']]\n",
    "Y_02 = X_T_Y.loc[X_T_Y['T'].isin(['0', '2'])][['Y']]\n",
    "\n",
    "T_02['T'] = T_02['T'].replace({'2': '1'}) \n",
    "\n",
    "X_T_01 = pd.concat([X_01, T_01], axis=1)\n",
    "X_T_02 = pd.concat([X_02, T_02], axis=1) \n",
    "\n",
    "T_01['T'] = T_01['T'].astype(np.int64)\n",
    "T_01 = np.array(T_01['T'])\n",
    "T_02['T'] = T_02['T'].astype(np.int64)\n",
    "T_02 = T_02.values.reshape(-1)\n",
    "\n",
    "X_T_01['T'] = X_T_01['T'].astype(np.int64) \n",
    "X_Y_01 = pd.concat((X_01, Y_01), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767ddbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_01_BAL = X_01\n",
    "T_01_BAL = T_01\n",
    "Y_01_BAL = Y_01 \n",
    "X_T_01_BAL = pd.concat((X_01_BAL.reset_index(drop=True), pd.DataFrame({\"T\":T_01_BAL})), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7affa177",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02_BAL = X_02\n",
    "T_02_BAL = T_02\n",
    "Y_02_BAL = Y_02 \n",
    "X_T_02_BAL = pd.concat((X_02_BAL.reset_index(drop=True), pd.DataFrame({\"T\":T_02_BAL})), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b46e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0526ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# para_XT01 = optunaSearchCVParams_LGBM(X_01_BAL, pd.DataFrame(T_01_BAL), objective='binary', scoring='roc_auc_ovr_weighted')\n",
    "para_XT01 = {'boosting_type': 'gbdt', 'objective': 'binary', 'class_weight': 'balanced', 'n_jobs': -1, 'learning_rate': 0.03, 'min_child_samples': 7, 'n_estimators': 500, 'num_leaves': 7, 'reg_lambda': 0.05, 'seed': 42}\n",
    "mdl_t01 = lgb.LGBMClassifier(**para_XT01) \n",
    "# binary\n",
    "# Best parameters found by grid search are: {'boosting_type': 'gbdt', 'objective': 'binary', 'class_weight': 'balanced', 'n_jobs': -1, 'learning_rate': 0.03, 'min_child_samples': 7, 'n_estimators': 500, 'num_leaves': 7, 'reg_lambda': 0.05, 'seed': 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3fb9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para_XTY01 = optunaSearchCVParams_LGBM(X_T_01_BAL, Y_01_BAL, objective='regression', scoring='neg_root_mean_squared_error')\n",
    "para_XTY01 = {'boosting_type': 'gbdt', 'n_jobs': -1, 'learning_rate': 0.05, 'min_child_samples': 20, 'n_estimators': 3000, 'num_leaves': 7, 'objective': 'regression', 'reg_lambda': 0.01, 'seed': 42}\n",
    "mdl_y01 = lgb.LGBMRegressor(**para_XTY01) \n",
    "# regression\n",
    "# Best parameters found by grid search are: {'boosting_type': 'gbdt', 'n_jobs': -1, 'learning_rate': 0.05, 'min_child_samples': 20, 'n_estimators': 3000, 'num_leaves': 7, 'objective': 'regression', 'reg_lambda': 0.01, 'seed': 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1af13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "68645508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'min_leaf_size': 60, 'max_depth': 8, 'subsample_ratio': 0.35, 'min_balance': 0.45, 'max_feat': 0.9} 0.36057932254417746\n"
     ]
    }
   ],
   "source": [
    "n_estimators_list = [500]# fitted\n",
    "min_leaf_size_list = [60]# 20-90\n",
    "subsample_ratio_list = [0.35]#fiteed\n",
    "max_depth_list = [8]#fitted \n",
    "max_features_list = [0.9]# fitted\n",
    "min_balancedness_tol_list = [0.45]#fitted, 0.4, 0.35, 0.3]\n",
    "for n_est in n_estimators_list: \n",
    "    for min_leaf in min_leaf_size_list:\n",
    "        for subsample_ratio in subsample_ratio_list:\n",
    "            for max_dep in max_depth_list:\n",
    "                for min_balance in min_balancedness_tol_list:\n",
    "                    for max_feat in max_features_list:\n",
    "                        para_tmp = {'n_estimators': n_est, \n",
    "                                   'min_leaf_size': min_leaf, \n",
    "                                   'max_depth': max_dep, \n",
    "                                   'subsample_ratio': subsample_ratio, \n",
    "                                   'min_balance': min_balance, \n",
    "                                   'max_feat': max_feat} \n",
    "                    # 01 \n",
    "                        est01 = ForestDRLearner(\n",
    "                                        model_regression=mdl_y01, \n",
    "                                        model_propensity=mdl_t01, \n",
    "                                        featurizer=None, \n",
    "                                        min_propensity=1e-06, \n",
    "                                        categories=[0, 1], \n",
    "                                        cv=2,\n",
    "                                        mc_iters=4,\n",
    "                                        mc_agg='mean', \n",
    "                                        n_estimators=n_est, \n",
    "                                        max_depth=max_dep, \n",
    "#                                         min_samples_split=5, \n",
    "                                        min_samples_leaf=min_leaf, \n",
    "#                                         min_weight_fraction_leaf=0.0, \n",
    "                                        max_features=max_feat, \n",
    "#                                         min_impurity_decrease=0.0, \n",
    "                                        max_samples=subsample_ratio, \n",
    "                                        min_balancedness_tol=min_balance, \n",
    "                                        honest=True, \n",
    "                                        subforest_size=4, \n",
    "                                        n_jobs = -1, \n",
    "                                        verbose=0, \n",
    "                                        random_state=0)\n",
    "                    \n",
    "                        # fit\n",
    "                        est01.fit(Y=Y_01_BAL, T=T_01_BAL, X=X_01_BAL)\n",
    "                        # effect\n",
    "                        test_t01 = est01.effect(X=test, T0=0, T1=1) \n",
    "                        X_t01 = est01.effect(X=X, T0=0, T1=1)\n",
    "                        print(para_tmp, calc_score(X_t01.reshape(-1), test_t01.reshape(-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'n_estimators': 500, 'min_leaf_size': 80, 'max_depth': 7, 'subsample_ratio': 0.35, \n",
    "#  'min_balance': 0.45, 'max_feat': 0.9} 0.37243761942135306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6f649ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_t01,columns=['X_t01']).to_csv('./dataset/model/forestDRlearner/X_t01.csv',index=False) \n",
    "# pd.DataFrame(test_t01,columns=['test_t01']).to_csv('./dataset/model/forestDRlearner/test_t01.csv',index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a88725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.36  {'n_estimators': 500, 'min_leaf_size': 60, 'max_depth': 8, 'subsample_ratio': 0.35, 'min_balance': 0.45, 'max_feat': 0.9} 0.36057932254417746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79f1a20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/model/forestDRlearner/est01.joblib']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from joblib import dump\n",
    "# dump(est01, './dataset/model/forestDRlearner/est01.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a49864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1baaabd",
   "metadata": {},
   "source": [
    "##### est02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4888931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary\n",
      "Best parameters found by grid search are: {'boosting_type': 'gbdt', 'objective': 'binary', 'class_weight': 'balanced', 'n_jobs': -1, 'learning_rate': 0.01, 'min_child_samples': 3, 'n_estimators': 500, 'num_leaves': 7, 'reg_lambda': 0, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "para_XT02 = bayesGridSearchCVParams(X_02_BAL, pd.DataFrame(T_02_BAL), \n",
    "                                    objective='binary', scoring='roc_auc_ovr_weighted')\n",
    "mdl_t02 = lgb.LGBMClassifier(**para_XT02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2cff710e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression\n",
      "Best parameters found by grid search are: {'boosting_type': 'gbdt', 'n_jobs': -1, 'learning_rate': 0.05, 'min_child_samples': 20, 'n_estimators': 3000, 'num_leaves': 7, 'objective': 'regression', 'reg_lambda': 0.05, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "para_XTY02 = bayesGridSearchCVParams(X_T_02_BAL, Y_02_BAL, \n",
    "                                     objective='regression', scoring='neg_root_mean_squared_error')\n",
    "mdl_y02 = lgb.LGBMRegressor(**para_XTY02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dd8db718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 300, 'min_leaf_size': 20, 'max_depth': 12, 'subsample_ratio': 0.33, 'min_balance': 0.45, 'max_feat': 0.9} 0.05430872390981582\n"
     ]
    }
   ],
   "source": [
    "n_estimators_list = [300]# fitted\n",
    "min_leaf_size_list = [20]# 20-90?\n",
    "subsample_ratio_list = [0.33]#fiteed\n",
    "max_depth_list = [12]#fitted \n",
    "max_features_list = [0.9]# fitted\n",
    "min_balancedness_tol_list = [0.45]#fitted, 0.4, 0.35, 0.3]\n",
    "for n_est in n_estimators_list: \n",
    "    for min_leaf in min_leaf_size_list:\n",
    "        for subsample_ratio in subsample_ratio_list:\n",
    "            for max_dep in max_depth_list:\n",
    "                for min_balance in min_balancedness_tol_list:\n",
    "                    for max_feat in max_features_list:\n",
    "                        para_tmp = {'n_estimators': n_est, \n",
    "                                   'min_leaf_size': min_leaf, \n",
    "                                   'max_depth': max_dep, \n",
    "                                   'subsample_ratio': subsample_ratio, \n",
    "                                   'min_balance': min_balance, \n",
    "                                   'max_feat': max_feat} \n",
    "                    # 01 \n",
    "                        est02 = ForestDRLearner(\n",
    "                                        model_regression=mdl_y02, \n",
    "                                        model_propensity=mdl_t02, \n",
    "                                        featurizer=None, \n",
    "                                        min_propensity=1e-06, \n",
    "                                        categories=[0, 1], \n",
    "                                        cv=2,\n",
    "                                        mc_iters=5,\n",
    "                                        mc_agg='mean', \n",
    "                                        n_estimators=n_est, \n",
    "                                        max_depth=max_dep, \n",
    "#                                         min_samples_split=5, \n",
    "                                        min_samples_leaf=min_leaf, \n",
    "#                                         min_weight_fraction_leaf=0.0, \n",
    "                                        max_features=max_feat, \n",
    "#                                         min_impurity_decrease=0.0, \n",
    "                                        max_samples=subsample_ratio, \n",
    "                                        min_balancedness_tol=min_balance, \n",
    "                                        honest=True, \n",
    "                                        subforest_size=4, \n",
    "                                        n_jobs = -1, \n",
    "                                        verbose=0, \n",
    "                                        random_state=0)\n",
    "                    \n",
    "                        # fit\n",
    "                        est02.fit(Y=Y_02_BAL, T=T_02_BAL, X=X_02_BAL)\n",
    "                        # effect\n",
    "                        test_t02 = est02.effect(X=test, T0=0, T1=1) \n",
    "                        X_t02 = est02.effect(X=X, T0=0, T1=1)\n",
    "                        print(para_tmp, calc_score02(X_t02.reshape(-1), test_t02.reshape(-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dfea7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_t02, columns=['X_t02']).to_csv('./dataset/model/forestDRlearner/X_t02.csv',index=False) \n",
    "# pd.DataFrame(test_t02, columns=['test_t02']).to_csv('./dataset/model/forestDRlearner/test_t02.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f5008845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/model/forestDRlearner/est02.joblib']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from joblib import dump\n",
    "# dump(est02, './dataset/model/forestDRlearner/est02.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30bff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d1e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4431d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36747697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econml",
   "language": "python",
   "name": "econml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
